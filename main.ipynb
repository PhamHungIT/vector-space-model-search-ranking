{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data-20220511T090048Z-001/data/'\n",
    "num_files = 6359\n",
    "\n",
    "def load_data6359(load_path):\n",
    "    \"\"\"\n",
    "    Input: Load path of data input\n",
    "    Output: An list of strings correspond to data input\n",
    "\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for i in range(1, num_files + 1):\n",
    "        with open(load_path + 'news' + str(i).zfill(5) + '.txt') as f:\n",
    "            contents = f.read()\n",
    "            corpus.append(contents)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def tf_idf(corpus):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=corpus)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X\n",
    "\n",
    "def cosine_similarity(vec1 , vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "    \n",
    "def vec(load_path):\n",
    "    corpus = load_data6359(load_path)\n",
    "    return tf_idf(corpus)\n",
    "\n",
    "corpus = load_data6359(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "Tokenizing, removing stop words and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_tokenized_list(doc_text):\n",
    "    \"\"\"\n",
    "    Return a list tokenized and of any text \n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "\n",
    "def word_stemmer(token_list):\n",
    "    \"\"\"\n",
    "    Return a list of word stemmed on tokenized words\n",
    "    \"\"\"\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for word in token_list:\n",
    "        stemmed.append(ps.stem(word))\n",
    "    return stemmed\n",
    "\n",
    "def get_stopwords_vietnamesse(path):\n",
    "    \"\"\"\n",
    "    Get data that contain stopwords in Vietnamese\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "def remove_stopwords(doc_text):\n",
    "    \"\"\"\n",
    "    Return a list of word after remove stopwords\n",
    "    \"\"\"\n",
    "    stopwords_vn = get_stopwords_vietnamesse('stopword_vn.txt')\n",
    "    removed_stopwords = []\n",
    "    for word in doc_text:\n",
    "        if word not in stopwords_vn:\n",
    "            removed_stopwords.append(word)\n",
    "    return removed_stopwords\n",
    "\n",
    "def clean_word(text):\n",
    "    \"\"\"\n",
    "    Return clean text after stemming and removing stopwords\n",
    "    \"\"\"\n",
    "    token_list = get_tokenized_list(text)\n",
    "    stemmed = word_stemmer(token_list)\n",
    "    cleaned = remove_stopwords(stemmed)\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nClean and save doc data\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clean and save doc data\n",
    "\n",
    "\"\"\"\n",
    "# id = 0\n",
    "# # clean doc data\n",
    "# for text in corpus:\n",
    "#     cleaned = clean_word(text)\n",
    "#     id+=1 \n",
    "#     file_out = open(\"./clean_doc_data/news\" + str(id).zfill(5) + \".txt\", \"w\")\n",
    "#     file_out.write(cleaned)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data6359(\"./clean_doc_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Duplicate term in vocabulary: \"tàu điện cát linh - hà đông thể vận hành ? khăn hiện dự án cát linh - hà đông huy động chuyên gia việt nam hoàn thành đánh giá an toàn hệ thống . dự án đường sắt cát linh - hà đông khởi công 10/2011 , dự kiến vận hành quý ii/2019 , xác định ngài vận hành thức . hồi 6 , thủ tướng giao giao thông vận tải hoàn thiện , khai thác tuyến đường sắt đô thị cát linh - hà đông 2020 ; báo cáo phủ vướng mắc dự án , trình quốc hội hướng xử lý . tuyến đường chiều 13,05 km 12 ga 1 khu depot . tổng đầu tư ban đầu 8.770 tỷ đồng ( 553 triệu usd ) , chỉnh 18.002 tỷ đồng ( 868 triệu usd ) . nài , vốn vay oda trung quốc 13.867 tỷ đồng , vốn đối ứng 4.134 tỷ . nhiên , dự án nài tồn , vướng mắc liên quan thiết khu depot , đánh giá an toàn đoàn tàu , công tác vận hành toàn hệ thống quyết toán ... giao thông vận tải đạo giải quyết vướng mắc nài hoàn thiện hạng mục . giải quyết `` khăn '' nêu hoàn thành công tác đánh giá an toàn hệ thống . giao thông vận tải đề nghị ngoại giao công hàm đề nghị quan chức năng pháp hỗ trợ , tháo gỡ , giúp chuyên gia tư vấn act ( tư vấn đánh giá an toàn hệ thống ) việt nam thực hiện dự án . phủ báo cáo liên quan lĩnh vực giao thông vận tải gửi đại biểu quốc hội kỳ họp 10 ( khai mạc 20/10 ) . , dự án đường sắt đô thị hà nội , tuyến cát linh - hà đông ( giao thông vận tải chủ đầu tư ) hoàn thành , hoàn thiện thủ tục nghiệm thu , bàn giao khai thác .\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hungpv3/Documents/learn/vector_space_model/main.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000020?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(data))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000020?line=7'>8</a>\u001b[0m \u001b[39m# TF-IDF\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000020?line=8'>9</a>\u001b[0m data_vec \u001b[39m=\u001b[39m tf_idf(data)\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000020?line=9'>10</a>\u001b[0m data_vec\n",
      "\u001b[1;32m/home/hungpv3/Documents/learn/vector_space_model/main.ipynb Cell 3'\u001b[0m in \u001b[0;36mtf_idf\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000002?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtf_idf\u001b[39m(corpus):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000002?line=18'>19</a>\u001b[0m     vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(vocabulary\u001b[39m=\u001b[39mcorpus)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000002?line=19'>20</a>\u001b[0m     X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(corpus)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hungpv3/Documents/learn/vector_space_model/main.ipynb#ch0000002?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2057'>2058</a>\u001b[0m \u001b[39m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2058'>2059</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2059'>2060</a>\u001b[0m \u001b[39mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2073'>2074</a>\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2074'>2075</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2075'>2076</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m-> <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2076'>2077</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2077'>2078</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2078'>2079</a>\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=2079'>2080</a>\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1314\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1308'>1309</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1309'>1310</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1310'>1311</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1312'>1313</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m-> <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1313'>1314</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_vocabulary()\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1314'>1315</a>\u001b[0m max_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_df\n\u001b[1;32m   <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=1315'>1316</a>\u001b[0m min_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_df\n",
      "File \u001b[0;32m~/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:473\u001b[0m, in \u001b[0;36m_VectorizerMixin._validate_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=470'>471</a>\u001b[0m         \u001b[39mif\u001b[39;00m vocab\u001b[39m.\u001b[39msetdefault(t, i) \u001b[39m!=\u001b[39m i:\n\u001b[1;32m    <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=471'>472</a>\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDuplicate term in vocabulary: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m t\n\u001b[0;32m--> <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=472'>473</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=473'>474</a>\u001b[0m     vocabulary \u001b[39m=\u001b[39m vocab\n\u001b[1;32m    <a href='file:///home/hungpv3/anaconda3/envs/work_fun/lib/python3.9/site-packages/sklearn/feature_extraction/text.py?line=474'>475</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate term in vocabulary: \"tàu điện cát linh - hà đông thể vận hành ? khăn hiện dự án cát linh - hà đông huy động chuyên gia việt nam hoàn thành đánh giá an toàn hệ thống . dự án đường sắt cát linh - hà đông khởi công 10/2011 , dự kiến vận hành quý ii/2019 , xác định ngài vận hành thức . hồi 6 , thủ tướng giao giao thông vận tải hoàn thiện , khai thác tuyến đường sắt đô thị cát linh - hà đông 2020 ; báo cáo phủ vướng mắc dự án , trình quốc hội hướng xử lý . tuyến đường chiều 13,05 km 12 ga 1 khu depot . tổng đầu tư ban đầu 8.770 tỷ đồng ( 553 triệu usd ) , chỉnh 18.002 tỷ đồng ( 868 triệu usd ) . nài , vốn vay oda trung quốc 13.867 tỷ đồng , vốn đối ứng 4.134 tỷ . nhiên , dự án nài tồn , vướng mắc liên quan thiết khu depot , đánh giá an toàn đoàn tàu , công tác vận hành toàn hệ thống quyết toán ... giao thông vận tải đạo giải quyết vướng mắc nài hoàn thiện hạng mục . giải quyết `` khăn '' nêu hoàn thành công tác đánh giá an toàn hệ thống . giao thông vận tải đề nghị ngoại giao công hàm đề nghị quan chức năng pháp hỗ trợ , tháo gỡ , giúp chuyên gia tư vấn act ( tư vấn đánh giá an toàn hệ thống ) việt nam thực hiện dự án . phủ báo cáo liên quan lĩnh vực giao thông vận tải gửi đại biểu quốc hội kỳ họp 10 ( khai mạc 20/10 ) . , dự án đường sắt đô thị hà nội , tuyến cát linh - hà đông ( giao thông vận tải chủ đầu tư ) hoàn thành , hoàn thiện thủ tục nghiệm thu , bàn giao khai thác .\""
     ]
    }
   ],
   "source": [
    "# Load and clean query\n",
    "with open('query.txt') as f:\n",
    "    query = f.read()\n",
    "cleaned_query = clean_word(query)\n",
    "data.append(cleaned_query)\n",
    "print(len(data))\n",
    "\n",
    "# TF-IDF\n",
    "data_vec = tf_idf(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36399/3779860881.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=[('id', '<i8'), ('similarity', '<f8')])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = [('id', int), ('similarity', float)]\n",
    "id_rel = []\n",
    "for i in range(len(data_vec)-1):\n",
    "    x = cosine_similarity(data_vec[-1], data_vec[i,:])\n",
    "    if x > 0: \n",
    "        id_rel .append((i, x))\n",
    "id_rel = np.array(id_rel, dtype=dtype)\n",
    "id_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respond for query\n",
    "num_responds = 10\n",
    "\n",
    "\n",
    "respond= open(\"respond.txt\", \"w\")\n",
    "id_rel_sorted = np.sort(id_rel, order='similarity')[::-1]\n",
    "respond.write(f\"Top {num_responds} kết quả tốt nhất: \\n\\n\")\n",
    "for id in enumerate(id_rel_sorted[:num_responds]):\n",
    "    respond.write(f\"Rank: {id[0]+1} - Score: {id[1][1]} \\n {corpus[id[1][0]]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a921b4654642c77e80c066082c58bfe216a192a047c034363c47413491dc481a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('work_fun')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
